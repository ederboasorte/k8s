


--Pre requisitos (control-pane e data-plane)
vi /etc/fstab
swapoff -a

cat <<EOF | tee /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
EOF

sysctl --system

echo "br_netfilter" >> /etc/modules-load.d/modules.conf
modprobe br_netfilter
lsmod | grep -i br_netfilter

apt-get update
apt-get install ca-certificates curl
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg -o /etc/apt/keyrings/docker.asc
chmod a+r /etc/apt/keyrings/docker.asc

echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/debian \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  tee /etc/apt/sources.list.d/docker.list > /dev/null

apt-get update
apt-get install -y containerd.io 
apt-get install -y docker-ce docker-ce-cli docker-buildx-plugin docker-compose-plugin

containerd config default > /etc/containerd/config.toml
systemctl restart containerd
systemctl status containerd

apt-get install -y apt-transport-https ca-certificates curl gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.34/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.34/deb/ /' | tee /etc/apt/sources.list.d/kubernetes.list

apt-get update
apt-get install -y kubelet kubeadm kubectl
apt-mark hold kubelet kubeadm kubectl
systemctl enable --now kubelet
dpkg -l | grep kube


--Validar cgroup sta usando (systemctl que esta como o mesmo cgroup abaixo)
mount | grep -i cgroup
--saida: cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime)

--configurar cgroup
vi /etc/containerd/config.toml
SystemdCgroup = true

systemctl restart containerd
systemctl status containerd





--instalar cluster com kubeadm (apenas no control-plane)
--OBS: adicionar o nome do servidor no /etc/hosts
vi /etc/hosts
192.168.15.210 control-plane

vi kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.15.210
  bindPort: 6443
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
networking:
  podSubnet: "10.244.0.0/16"
---  
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd


kubeadm --config kubeadm-config.yaml

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

--validar ps containers em execução via containerd
crictl ps
crictl logs -f 824dce7f424c7

--instalar driver de rede
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

--Adicionar nós data-plane ao cluster (rodar no control-plane)
kubeadm token create --print-join-command









--kubeconfig
export KUBECONFIG=$HOME/.kube/<nome_do_config>
ou 
KUBECONFIG=$HOME/.kube/<nome_do_config> kubectl get po -A
ou
kubectl get po -A --kubeconfig <nome_do_config>

--validar
env | grep KUBE

--remover variavel
unset KUBECONFIG






--ETCD
apt-get install etcd-client
etcdctl
cd /etc/kubernetes/pki/etcd
ETCDDCTL_API=3 etcdctl --cacert $(pwd)/ca.crt --cert $(pwd)/server.crt --key $(pwd)/server.key member list --write-out table

vi etcdctl.env
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key
export ETCDCTL_API=3

source etcdctl.env


--Backup etcd
cd /var/lib/etcd #pode salvar em qualquer lugar
etcdctl snapshot save snapshot.db

--Restore etcd
cd /var/lib/etcd 
etcdctl snapshot restore snapshot.db







--sintaxe do yaml
kubectl explain deployment
kubectl explain deployment.metadata
kubectl explain deployment.spec






--krew como instalar
apt install git

(
  set -x; cd "$(mktemp -d)" &&
  OS="$(uname | tr '[:upper:]' '[:lower:]')" &&
  ARCH="$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/')" &&
  KREW="krew-${OS}_${ARCH}" &&
  curl -fsSLO "https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz" &&
  tar zxvf "${KREW}.tar.gz" &&
  ./"${KREW}" install krew
)

vi .profile
export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"

kubectl krew install neat

kubectl get po -n kube-flannel kube-flannel-ds-qrdpf -oyaml | kubectl neat









--Namespace
kubectl create ns vicosa --dry-run=client -oyaml | kubectl neat > na.yaml
kubectl apply -f ns.yaml

kubectl run meu-pod --image=nginx:latest --dry-run=client -oyaml | kubectl neat > pod.yaml
vi pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: meu-pod
  namespace: vicosa
spec:
  containers:
  - image: nginx:latest
    name: meu-pod

kubectl apply -f pod.yaml
kubectl get po -n vicosa












--Initcontainer
vi pod_initcontainer.yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-cd
  namespace: default
spec:
  containers:
  - image: nginx:1.14.2
    name: nginx
	ports:
	- containerPort: 80
  initContainers:
  - name: waitfordns
    image: busybox:1.28
	command:
	- sh
	- -c
	- until nslookup mymysql; do echo "Tentando resolver nome"; sleep 2; done
	
kubectl apply -f pod_initcontainer.yaml
	
--validar logs do Pod
kubectl logs -f nginx-cd -n default

--validar logs do container dentro do pod
kubectl logs -f nginx-cd -n default -c waitfordns










--Multi Containers
vi pod_multicontainers.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: multcontainer-pod
  name: multcontainer-pod
spec:
  containers:
  - image: httpd
    name: httpd
  - image: alpine
    name: alpine
    command:
      - "sleep"
      - "9999999"
	  
kubectl apply -f pod_multicontainers.yaml
	
--validar logs do Pod
kubectl logs -f multcontainer-pod -n default
kubectl dexcribe pod multcontainer-pod -n default

--validar logs do container dentro do pod
kubectl logs -f multcontainer-pod -n default -c httpd
kubectl logs -f multcontainer-pod -n default -c apline

--Entrar nos containers
kubectl multcontainer-pod -n default -ti -c httpd -- sh
kubectl multcontainer-pod -n default -ti -c apline -- sh 







--Deployment
--Verificar qual apiVersao usar
kubectl api-resource | grep -i deploy

--Gerar um exemplo de Deployment
kubectl create deploy --image httpd httpd
kubectl get deploy httpd -o yaml | kubectl neat > deployment_httpd.yaml
ou
kubectl create deploy --image httpd httpd --replicas 3 --dry-run=client -o yaml | kubectl neat >  deployment_httpd.yaml
cat deployment_httpd.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: httpd
  name: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
    spec:
      containers:
      - image: httpd
        name: httpd

kubectl apply -f deployment_httpd.yaml
kubectl get deploy
kubectl get replicaset 
kubectl get po
kubectl get replicasets









--Labels e selectors
kubectl get deploy --show-labels
kubectl get po --show-labels

cat deployment_httpd.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: httpd
    ambiente: producao
  name: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
        ambiente: producao
    spec:
      containers:
      - image: httpd
        name: httpd

kubectl apply -f deployment_httpd.yaml
kubectl get deploy --show-labels
kubectl get deploy -l ambiente=producao
kubectl get po --show-labels
kubectl get po -l ambiente=producao


--Adicionar label manualmente em um POD, Deploy etc
kubectl label pod <nome_do_pod> app=nginx
kubectl get deployment --show-labels









--Rollout
--histoty
kubectl rollout history deployment/httpd

--Rollback
kubectl rollout undo deployment/httpd --to-revison=3

--restart
kubectl rollout restart deployment/httpd











--Strategia (strategy)
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: httpd
    ambiente: producao
  name: httpd
spec:
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
  replicas: 4
  selector:
    matchLabels:
      app: httpd
  template:
    metadata:
      labels:
        app: httpd
        ambiente: producao
    spec:
      containers:
      - image: httpd
        name: httpd








--Scalar manualmente um deploy
kubectl get po 
kubectl scale deploy httpd --replicas 2
kubectl get po 








--resource (limitar recursos)
request = inicia com x% de cpu ou memoria 
limits = maximo de recuros que o container pode usar

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
    ambiente: producao
  name: nginx
spec:
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        ambiente: producao
    spec:
      containers:
      - image: nginx
        name: nginx
         resources:
          requests:
            cpu: 100m
            memory: 56M
          limits:
            cpu: 200m
            memory: 128M

kubectl apply -f deployment_httpd.yaml
kubectl get po 
kubectl describe po httpd-5dc75759f6-q4w92
kubectl describe node data-plane-1








--Goldilocks (estimatica de recursos)
--documentação de instalação
https://goldilocks.docs.fairwinds.com/installation/#installation-2

--values
https://artifacthub.io/packages/helm/fairwinds-stable/goldilocks

--instalação
helm install goldilocks --namespace goldilocks --create-namespace fairwinds-stable/goldilocks -f values.yaml











--Probles (Self healing) (heathcheck)
readiness = Quando a aplicação esta pronta para receber o trafego (ocorre no start do pod)
Liveness = quando a aplicação já esta no ar, verifica se a aplicação esta saudavel, se começar a dar erro na aplicação o Liveness reinicia o pod

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
    ambiente: producao
  name: nginx
spec:
  strategy: 
    rollingUpdate:
      maxSurge: 10%
      maxUnavailable: 0
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        ambiente: producao
    spec:
      containers:
      - image: nginx
        name: nginx
        readinessProbe:
          httpGet:
            path: "/"
            port: 80
        livenessProbe:
          httpGet:
            path: "/"
            port: 80
        resources:
          requests:
            cpu: 100m
            memory: 56M
          limits:
            cpu: 200m
            memory: 128M










--Variaveis de ambiente
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
    ambiente: producao
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
        ambiente: producao
    spec:
      containers:
      - image: nginx
        name: nginx
        env:
          - name: ENVIROMENT
            value: producao
          - name: ALUNO
            value: Yasmim










--DaemonSets
--DaemonSet é um servico que roda em cada node, se tivermos 3 nodes, teremos e daemonsets um em cada node
kubectl api-resources | grep -i daemon
kubectl get ds -A
kubectl create deploy --image nginx nginx --dry-run -oyaml | kubectl neat > daemonset.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx

kubectl get ds
kubectl get po -owide









--Stateful e Stateless
--local path provisoner
https://github.com/rancher/local-path-provisioner
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.32/deploy/local-path-storage.yaml
kubectl get po -n local-path-storage
kubectl logs -f -n local-path-storage local-path-provisioner-866d54d4c8-fq9sl
kubectl get storageclass -A
--adicionar a annotation abaixo (torrnar o local-path como default)
kubectl edit storageclasses local-path
  storageclass.kubernetes.io/is-default-class: "true"
kubectl get storageclasses -A

apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: nginx-html
          mountPath: "/usr/shared/nginx/html"

  volumeClaimTemplates:
    - metadata:
        name: nginx-html      
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 1G

kubectl apply -f statefulset.yaml
kubectl get statefulset
kubectl get po 









--jobs (batch jobs)
kubectl api-resources | grep -i jobs
kubectl explain jobs
kubectl get job -A

vi job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: echo-command
spec:
  template:
    spec:
      containers:
      - name: sleep
        image: alphine
        command: 
          - echo
          - "Eder Vilela Brandão"
      restartPolicy: "OnFailure"
  #ttlSecondsAfterFinished: 0

kubectl apply -f job.yaml
kubectl get job 
kubectl get po 
kubectl logs -f echo-command-pz5kw



--Cronjob
https://crontab.guru/

kubectl get cronjob -A

vi cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: sleep
            image: alpine
            command: 
              - echo
              - "Eder Vilela Brandão"
          restartPolicy: "OnFailure"

kubectl apply -f cronjob.yaml
kubectl get cronjob -A
kubectl get job -A
kubectl get po -A








--Service
ClusterIP = Exporta sua aplicação para dentro do cluster kubernetes, Utilizado apenas para comunicação internado do cluster, não recebe requsição de fora do cluster
NodePort = (OBS: Não é muito utilizado), Exporta sua aplicação para fora do cluster, pega uma porta alta do Node (30000 - 32767)(sera a mesma porta em todos os nodes) e vincula a porta do pod
LoadBalancer = (Mais utilizado em cloud, se usar no baremetal precisa do metallb) 
ExternalName = Redirecionamento de dns para uma aplicação externa, quando utilizar? quando vc tem um banco de dados externo e sua aplicação dentro co cluster precisa acesso esse banco de dados via dns vc cria um ExternalName

kubectl api-resources | grep -i services
kubectl get services -A
kubectl get services kubernetes -n default -o yaml



--ClusterIP
vi deployment_nginx.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx


vi services_nginx.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
      app: nginx
  ports:
  - name: https
    port: 80
    targetPort: 80
  type: ClusterIP


kubectl apply -f deployment_nginx.yaml
kubectl aplly -f services_nginx.yaml
kubectl get po 
kubectl get svc -A

kubectl run --image alpine -it demo sh
apk add curl
curl nginx
curl nginx.default.svc.cluster.local
curl <ip_do_service>








--Criar service via linha de comando (expose)
kubectl expose deployment nginx -n default --port 80 --target-port 80
kubectl get services -A









--DNS
kubectl run --image alpine -it demo sh
apk add curl
apk add bind-tools
curl nginx
host nginx #saida: nginx.default.svc.cluster.local has address 10.99.33.179
curl nginx.default.svc.cluster.local





--Endpoints
kubectl get endpoints
kubectl get ep
kubectl get endpointslices
kubectl describe endpointslices nginx 



--nodePort
kubectl get services

vi services_nginx.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  selector:
      app: nginx
  ports:
  - name: https
    port: 80
    targetPort: 80
  type: NodePort

kubectl apply -f services_nginx.yaml
kubectl get services






--LoadBalancer
kubectl get services

services_nginx.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  selector:
      app: nginx
  ports:
    - port: 80
      targetPort: 80
  type: LoadBalancer
  internalTrafficPolicy: local
  externalTrafficPolicy: Cluster

kubectl apply -f services_nginx.yaml
kubectl get services







--metalLB
--documentação
https://metallb.universe.tf/installation/

--validar modo de execução (IPVS ou iptables)
kubectl get cm -n kube-system kube-proxy -o yaml | grep -i mode

kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.15.2/config/manifests/metallb-native.yaml
kubectl get po -n metallb-system
mkdir metallb
cd metallb

vi manifests.yaml
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: private-subnet-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.168.15.245-192.168.15.249
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: private-subnet-advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
  - private-subnet-pool

kubectl apply -f manifests.yaml
kubectl get svc







--Headless service (usado na exposição de aplicações statefulsets)
--Headless service expoe o serviço mas na faz o balanceamento de carga

vi statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: nginx
  name: nginx-master
spec:
  serviceName: nginx-headless
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        volumeMounts:
        - name: nginx-html
          mountPath: "/usr/shared/nginx/html"
  volumeClaimTemplates:
    - metadata:
        name: nginx-html      
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 1G

kubectl apply -f statefulset.yaml
kubectl get po

vi headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-headless
  labels:
    app: nginx
spec:
  selector:
      app: nginx
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP
  clusterIP: None

kubectl apply -f headless.yaml
kubectl get po svc
kubectl run --image alpine -it demo sh
apk add bind-tools
host kubernetes
host nginx-headless
host nginx-master-0.nginx-headless






--ExterrnalName
vi external-name.yaml
apiVersion: v1
kind: Service
metadata:
  name: blog
spec:
  type: ExternalName
  externalName: google.com

kubectl apply -f external-name.yaml
kubectl get svc 
kubectl run --image alpine -it demo sh
apk add bind-tools
host blog
host -t cname blog










--Ingress
--Documentação
https://github.com/kubernetes/ingress-nginx
https://kubernetes.io/docs/concepts/services-networking/ingress/

--Fluxo
LoadBalancer > ingress > services > pod


wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.14.0/deploy/static/provider/baremetal/deploy.yaml
mv deploy.yaml ingress.yaml

vi ingress.yaml #alterar de NodePort para LoadBalancer
De:
type: NodePort
Para:
type: LoadBalancer

kubectl apply -f ingress.yaml
kubectl get po -n ingress-nginx
kubectl get svc -n ingress-nginx
kubectl get ingressclass
kubectl edit ingressclass nginx
annotations:
  ingressclass.kubernetes.io/is-default-class: "true"

kubectl get ingressclass nginx -o yaml




---teste
kubectl get deployment
kubectl get po

vi deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx

kubectl apply -f deployment.yaml
kubectl get deployment
kubectl get po
kubectl get svc
kubectl get endpoints

vi services.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  selector:
      app: nginx
  ports:
    - port: 80
      targetPort: 80
  type: ClusterIP

kubectl apply -f services.yaml
kubectl get svc
kubectl get endpoints
kubectl get ingress

vi ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-service-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: nginx.demo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80

kubectl apply -f ingress.yaml
kubectl get ingress
kubectl get svc -n ingress-nginx ##pegar ip EXTERNAL-IP
kubectl logs -f svc/ingress-nginx-controller -n ingress-nginx
curl <EXTERNAL-IP> -I #deve retornar 404
curl <EXTERNAL-IP> -H "Host: nginx.demo.com" -I #deve retornar a pagina do nginx
kubectl get po -n ingress-nginx
kubectl logs -f -n ingress-controller ingress-nginx-controller-565c7596d-hdgt7








--Traefik como ingress controller
https://doc.traefik.io/traefik/getting-started/install-traefik/

helm repo add traefik https://traefik.github.io/charts
helm repo update
helm install traefik traefik/traefik --namespace traefik --create-namespace --set logs.access.enabled=true
kubectl get po -n traefik
kubectl get ingressclasses
kubectl get svc -n traefik
kubectl logs -f -n traefik traefik-5d44fddd44-2j2lp

vi ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-service-ingress
spec:
  ingressClassName: traefik
  rules:
  - host: nginx.demo.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80

kubectl apply -f ingress.yaml
kubectl get svc -n traefik #pegar o EXTERNAL-IP
curl 192.168.15.245 -I
curl 192.168.15.245 -H "Host: nginx.demo.com" -I








--Rewrite Path com Nginx
kubectl get ingress
kubectl get ingressclasses

vi ingress_resource.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-service-ingress
  annotations:
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /nginx(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: nginx
            port:
              number: 80

kubectl apply -f ingress.yaml
kubectl get ingress








--TLS (na camada do ingress)
--Documentação
https://kubernetes.io/docs/concepts/services-networking/ingress/#tls

--Gerar o certificado
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj '/CN=*.demo.com/O=edervilela' -addext 'subjectAltName = DNS:*.demo.com'
ls -lrt tls.crt tls.key
openssl x509 -in tls.crt -text

--Secret
kubectl get secret
kubectl create  secret tls demo-domain-secret --key tls.key --cert tls.crt
kubectl get secret
kubectl get secret demo-domain-secret -o yaml #pegar o conteudo do tsl.crt
echo "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURURENDQWpTZ0F3SUJBZ0lVZkx3THpsMHBmSzRwVW9RdTg3c1JhZkxtdDJRd0RRWUpLb1pJaHZjTkFRRUwKQlFBd0tqRVRNQkVHQTFVRUF3d0tLaTVrWl
cxdkxtTnZiVEVUTUJFR0ExVUVDZ3dLWldSbGNuWnBiR1ZzWVRBZQpGdzB5TlRFeE1qQXhNekF4TXpoYUZ3MHlOakV4TWpBeE16QXhNemhhTUNveEV6QVJCZ05WQkFNTUNpb3VaR1Z0CmJ5NWpiMjB4RXpBUkJnTlZCQW
9NQ21Wa1pYSjJhV3hsYkdFd2dnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUIKRHdBd2dnRUtBb0lCQVFEZ2pEcmFYV2UrQVpyUUExR1ZoOG9ZL1AxWUpGYUZSNktHNFlNUEVMeVJxNFRITDhkOQpUQ1RKQkowazg3Mm
VGaXF6TzlpUDVpVWNGLy9lV3hpUVc3VEgzbHN6ZlJjdmVOMy9BbWU5dGVTL094ZnI0TUJVCmpMRWgzbGZpMUxGZ3JLaUJTMkpmb0RCQ2xXVGYrd3ptbllTVTR3RHFld2tNQmlDMTVoVjFybFRkK01IZmIxT1EKdVlTKz
JmVzNzQm9BVGhOd1d6NDRQVXRRZmVWNERnMVBBMUlrZEd0eFk5UWNramhLTTRBZEVwb3Mzb2krNFNHSgpLTnQrekNRWFA3dTJVTU9zL2o1Y2duczVvYVJJUG1uL2FzcWlsZjJqMkpCLzdIRDlmanFRem52UmFvOWxyej
FqCkhXMWhUWThvdlg2ald6M0RnRnNTQWlvZVVDUmR4eDNqYjFiTEFnTUJBQUdqYWpCb01CMEdBMVVkRGdRV0JCUkIKQjAvdkpXQnphRnIrYjI1K0g4U25vVXo3ZFRBZkJnTlZIU01FR0RBV2dCUkJCMC92SldCemFGci
tiMjUrSDhTbgpvVXo3ZFRBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUJVR0ExVWRFUVFPTUF5Q0Npb3VaR1Z0Ynk1amIyMHdEUVlKCktvWklodmNOQVFFTEJRQURnZ0VCQU5pL0lTL1FuUTFIT0d3TThuRmt1ckRxd1FLM0
FaSS9vY0VXNzNZZmJnL0kKYW0zWlR4ZFIwWFVjVmhWQ0hBVkZSRDlmVUpzWnRzWTZLM1FLUXhONVovQ3dRaFBuSkhqMkRoYUVpTVVkaGlhegpMU25ZWUh3ZnVQbGt1bXBpNmhlbFB5emVobWFJQ0YxSHNXQUprbllabT
JzT2pOc3pRRWFHUGhXY1QvN2VaQjR1CmdLK3dQM3dETUk5Z20xVU9JUTVIZHhZb2swTHFqWDBITjBRK0wzMXJnQS9GeDQvejF6aUQwcmExNklGcjh1L1oKYW5YMFRqZXYyQ0pFRXVmOEdmWCt2NEVwajdFbldkZjJOaG
xDMnpSQVVRYUU5UkJXMm5EbzNTTDhsUlA4NjJJawpJQXlyUFJVRkY2SnNrd2xiaXFxTEhrUmlRQ3loSkF5OXo3SUNWNC9NQjlnPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==" | base64 -d


vi ingress_resource.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-service-ingress
spec:
  ingressClassName: nginx
  tls:
  - hosts:
      - "*.demo.com"
    secretName: demo-domain-secret
  rules:
  - host: "nginx.demo.com"
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: nginx
            port:
              number: 80
            
kubectl get ingress
kubectl describe ingress web-service-ingress
kubectl get svc -n ingress-nginx #pegar o EXTERNAL-IP
curl 192.168.15.246 -H "host: nginx.demo.com"
curl 192.168.15.246 -H "host: nginx.demo.com" -I
curl 192.168.15.246 -H "host: nginx.demo.com" -I -L
curl 192.168.15.246 -H "host: nginx.demo.com" -I -L -k
curl 192.168.15.246 -H "host: nginx.demo.com" -L -k -v









--ConfigMap
--Documentação
https://kubernetes.io/docs/concepts/configuration/configmap/#configmap-object

kubectl api-resources | grep -i configmap
kubectl get configmap -A
kubectl get configmap -n kube-system codedns -o yaml

--criar configmap via linha de comando (não usual, recomendado via yaml)
kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2 --from-literal=nome=yasmim
kubectl get configmap
kubectl get configmap my-config

--criar configmap via yaml
vi configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-demo
data:
  virtual_host: "vhost1.localhost.com"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginxc
  name: nginxc
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginxc
  template:
    metadata:
      labels:
        app: nginxc
    spec:
      containers:
      - image: nginx
        name: nginxc
        env:  
          - name: VIRTUAL_HOST
            valueFrom:
              configMapKeyRef:
                name: nginx-demo
                key: virtual_host

kubectl get configmap
kubectl get configmap nginx-demo -o yaml
kubectl exec -ti nginxc-556ffbb69b-h7zqh -- env | grep VIRTUAL_HOST





--configmap (subPath)
--monta o configmap em um arquivo /usr/share/nginx/html/index.html

vi configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-demo
data:
  virtual_host: "vhost1.localhost.com"
  index.html: |
    <html>
      <h1>Yasmim linda!!!</h1>
    </html>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginxc
  name: nginxc
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginxc
  template:
    metadata:
      labels:
        app: nginxc
    spec:
      containers:
      - image: nginx
        name: nginxc
        env:  
          - name: VIRTUAL_HOST
            valueFrom:
              configMapKeyRef:
                name: nginx-demo
                key: virtual_host
        volumeMounts:
        - name: index-html
          mountPath: /usr/share/nginx/html/index.html
          subPath: index.html       
      volumes:
        - name: index-html
          configMap:
            name: nginx-demo
            items:
            - key: "index.html"
              path: "index.html"  

kubectl apply -f configmap.yaml
kubectl get po -o wide
kubectl get configmap
kubectl get configmap nginx-demo -o yaml













--Secrets
--Documentação
https://kubernetes.io/docs/concepts/configuration/secret/#secret-type

--tipos de secrets
docker-registry = (kubernetes.io/dockerconfigjson) especifico para uso do docker registry e outros registrys
generic = (Opaque) generico chave e valor
tls = (kubernetes.io/tls) para cetificados 

kubectl create secret generic credentials --from-literal=usernamen=admin
kubectl get secret
kubectl get secret crredentials -oymal
echo -n "admin" | base64

apiVersion: v1
kind: Secret
metadata:
  name: credentials
type: kubernetes.io/basic-auth  
data:
  username: WWFzbWlt
  password: WWFzbWlt
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginxc
  name: nginxc
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginxc
  template:
    metadata:
      labels:
        app: nginxc
    spec:
      containers:
      - image: nginx
        name: nginxc
        env:  
          - name: USERNAME
            valueFrom:
              secretKeyRef:
                name: credentials
                key: username
          - name: PASSWORD
            valueFrom:
              secretKeyRef:
                name: credentials
                key: password
        volumeMounts:
        - name: credentials
          mountPath: "/tmp"
          readOnly: true        
      volumes:
        - name: credentials
          secret:
            secretName: credentials

kubectl apply -f secret.yaml
kubectl get secret
kubectl get secret credentials -o yaml
kubectl exec -ti nginxc-66688876b4-bzxdv -- ls -l /tmp